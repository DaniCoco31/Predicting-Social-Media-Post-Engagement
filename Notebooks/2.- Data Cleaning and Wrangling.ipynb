{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we will clean and wrangle the merged data from the previous notebook. We will handle missing values, filter the data based on specific conditions, and prepare the dataset for exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Merged Data\n",
    "We start by loading the merged data generated from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import boxcox\n",
    "from scipy.stats import boxcox\n",
    "# import numpy sqrt\n",
    "from numpy import sqrt, log1p\n",
    "#import the functions in another file with sys.path.append\n",
    "import sys\n",
    "sys.path.append('../SCR/')\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Data Loaded:\n",
      "        id                                        description  interactions  \\\n",
      "0  4364357  great evening with craig nicky and david thank...            25   \n",
      "1  1721285  i went to party to celebrate the s read about ...            56   \n",
      "\n",
      "  day_of_week time_of_day  following  followers  num_posts  \\\n",
      "0      Sunday       night        164        139         58   \n",
      "1     Tuesday       night       1653        892        830   \n",
      "\n",
      "   is_business_account              category  ...  embedded_1014  \\\n",
      "0                 True  diaries_&_daily_life  ...       0.211534   \n",
      "1                 True       fashion_&_style  ...       0.069635   \n",
      "\n",
      "   embedded_1015  embedded_1016  embedded_1017  embedded_1018  embedded_1019  \\\n",
      "0      -0.582293      -0.019784      -0.754162       1.617441       1.141060   \n",
      "1      -0.430762      -0.083447      -0.734784       0.570494       0.623253   \n",
      "\n",
      "   embedded_1020  embedded_1021  embedded_1022  embedded_1023  \n",
      "0       0.021745      -0.881142       0.407194       0.323486  \n",
      "1       0.418022      -0.629205       0.396398      -0.498219  \n",
      "\n",
      "[2 rows x 1034 columns]\n",
      "Shape of the merged data: (10000, 1034)\n"
     ]
    }
   ],
   "source": [
    "# Load the merged data\n",
    "df = pd.read_csv('../Data/Clean-Data/merged_data.csv')\n",
    "print(\"Merged Data Loaded:\")\n",
    "print(df.head(2))\n",
    "print(\"Shape of the merged data:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data: We load the merged dataset to begin the cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Handling Missing Values\n",
    "We will check for and handle any missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " id               0\n",
      "description      0\n",
      "interactions     0\n",
      "day_of_week      0\n",
      "time_of_day      0\n",
      "                ..\n",
      "embedded_1019    0\n",
      "embedded_1020    0\n",
      "embedded_1021    0\n",
      "embedded_1022    0\n",
      "embedded_1023    0\n",
      "Length: 1034, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows with missing descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['description'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after handling missing values:\n",
      "(10000, 1034)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data after handling missing values:\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Missing Values: Critical for ensuring the dataset's integrity before further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Filtering Data\n",
    "We'll filter the data based on specific criteria, such as the number of followers, posts, and description length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter based on followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[(df['followers'] > 500) & (df['followers'] < 1500)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter based on the number of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df['num_posts'] >= 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter based on description length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[(df['description'].apply(len) >= 50) & (df['description'].apply(len) <= 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after filtering:\n",
      "(10000, 1034)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data after filtering:\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['diaries_&_daily_life', 'fashion_&_style', 'music',\n",
       "       'news_&_social_concern', 'relationships', 'food_&_dining',\n",
       "       'arts_&_culture', 'fitness_&_health', 'film_tv_&_video',\n",
       "       'learning_&_educational', 'travel_&_adventure', 'family',\n",
       "       'other_hobbies', 'sports', 'celebrity_&_pop_culture',\n",
       "       'business_&_entrepreneurs', 'science_&_technology', 'gaming',\n",
       "       'youth_&_student_life'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the unique values of the column category in the dataframe\n",
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering: Applying filters to refine the dataset to the most relevant rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Removing outliers to further statistics.\n",
    "Setting the data ready for EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'description', 'interactions', 'day_of_week', 'time_of_day',\n",
       "       'following', 'followers', 'num_posts', 'is_business_account',\n",
       "       'category',\n",
       "       ...\n",
       "       'embedded_1014', 'embedded_1015', 'embedded_1016', 'embedded_1017',\n",
       "       'embedded_1018', 'embedded_1019', 'embedded_1020', 'embedded_1021',\n",
       "       'embedded_1022', 'embedded_1023'],\n",
       "      dtype='object', length=1034)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the description column, we will calculate the length of the description\n",
    "df['description_length'] = df['description'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers and normalize relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1035)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_outliers(df, 'followers')\n",
    "df = remove_outliers(df, 'interactions')\n",
    "df = remove_outliers(df, 'num_posts')\n",
    "df = remove_outliers(df, 'description_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all values are positive where needed (especially for 'followers' and 'interactions')\n",
    "df['followers'] += 1\n",
    "df['interactions'] += 1\n",
    "df['num_posts'] += 1\n",
    "df['description_length'] += 1  # Assuming description length can be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Apply Square Root Transformation\\ndf['followers_trans'] = np.sqrt(df['followers'])\\ndf['interactions_trans'] = np.sqrt(df['interactions'])\\ndf['num_posts_trans'] = np.sqrt(df['num_posts'])\\ndf['description_length_trans'] = np.sqrt(df['description_length'])\\n\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' # Apply Square Root Transformation\n",
    "df['followers_trans'] = np.sqrt(df['followers'])\n",
    "df['interactions_trans'] = np.sqrt(df['interactions'])\n",
    "df['num_posts_trans'] = np.sqrt(df['num_posts'])\n",
    "df['description_length_trans'] = np.sqrt(df['description_length'])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Apply Box-Cox Transformation\\ndf['followers_trans'], _ = boxcox(df['followers'])\\ndf['interactions_trans'], _ = boxcox(df['interactions'])\\ndf['num_posts_trans'], _ = boxcox(df['num_posts'])\\ndf['description_length_trans'], _ = boxcox(df['description_length'])\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Apply Box-Cox Transformation\n",
    "df['followers_trans'], _ = boxcox(df['followers'])\n",
    "df['interactions_trans'], _ = boxcox(df['interactions'])\n",
    "df['num_posts_trans'], _ = boxcox(df['num_posts'])\n",
    "df['description_length_trans'], _ = boxcox(df['description_length'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Log Transformation\n",
    "df['followers_trans'] = np.log1p(df['followers'])\n",
    "df['interactions_trans'] = np.log1p(df['interactions'])\n",
    "df['num_posts_trans'] = np.log1p(df['num_posts'])\n",
    "df['description_length_trans'] = np.log1p(df['description_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Apply Square Root Transformation\\ndf['followers_sqrt'] = np.sqrt(df['followers'])\\ndf['interactions_sqrt'] = np.sqrt(df['interactions'])\\ndf['num_posts_sqrt'] = np.sqrt(df['num_posts'])\\ndf['description_length_sqrt'] = np.sqrt(df['description_length'])\\n\\n# Apply Log Transformation\\ndf['followers_log'] = np.log(df['followers'])\\ndf['interactions_log'] = np.log(df['interactions'])\\ndf['num_posts_log'] = np.log(df['num_posts'])\\ndf['description_length_log'] = np.log(df['description_length'])\\n\\n# Apply Box-Cox Transformation\\ndf['followers_boxcox'], _ = boxcox(df['followers'])\\ndf['interactions_boxcox'], _ = boxcox(df['interactions'])\\ndf['num_posts_boxcox'], _ = boxcox(df['num_posts'])\\ndf['description_length_boxcox'], _ = boxcox(df['description_length'])\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''# Apply Square Root Transformation\n",
    "df['followers_sqrt'] = np.sqrt(df['followers'])\n",
    "df['interactions_sqrt'] = np.sqrt(df['interactions'])\n",
    "df['num_posts_sqrt'] = np.sqrt(df['num_posts'])\n",
    "df['description_length_sqrt'] = np.sqrt(df['description_length'])\n",
    "\n",
    "# Apply Log Transformation\n",
    "df['followers_log'] = np.log(df['followers'])\n",
    "df['interactions_log'] = np.log(df['interactions'])\n",
    "df['num_posts_log'] = np.log(df['num_posts'])\n",
    "df['description_length_log'] = np.log(df['description_length'])\n",
    "\n",
    "# Apply Box-Cox Transformation\n",
    "df['followers_boxcox'], _ = boxcox(df['followers'])\n",
    "df['interactions_boxcox'], _ = boxcox(df['interactions'])\n",
    "df['num_posts_boxcox'], _ = boxcox(df['num_posts'])\n",
    "df['description_length_boxcox'], _ = boxcox(df['description_length'])'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'description', 'interactions', 'day_of_week', 'time_of_day',\n",
       "       'following', 'followers', 'num_posts', 'is_business_account',\n",
       "       'category',\n",
       "       ...\n",
       "       'embedded_1019', 'embedded_1020', 'embedded_1021', 'embedded_1022',\n",
       "       'embedded_1023', 'description_length', 'followers_trans',\n",
       "       'interactions_trans', 'num_posts_trans', 'description_length_trans'],\n",
       "      dtype='object', length=1039)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Output the first few rows to verify transformations\\nprint(df[['followers_sqrt', 'followers_log', 'followers_boxcox',\\n          'interactions_sqrt', 'interactions_log', 'interactions_boxcox',\\n          'num_posts_sqrt', 'num_posts_log', 'num_posts_boxcox',\\n          'description_length_sqrt', 'description_length_log', 'description_length_boxcox']].head())\\n          \""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Output the first few rows to verify transformations\n",
    "print(df[['followers_sqrt', 'followers_log', 'followers_boxcox',\n",
    "          'interactions_sqrt', 'interactions_log', 'interactions_boxcox',\n",
    "          'num_posts_sqrt', 'num_posts_log', 'num_posts_boxcox',\n",
    "          'description_length_sqrt', 'description_length_log', 'description_length_boxcox']].head())\n",
    "          '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6738, 1039)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the cleaned data for the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../Data/Clean-Data/cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column Management: Dropping irrelevant columns simplifies the dataset for subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The cleaned dataset is saved as cleaned_data.csv for the next stage of the pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
